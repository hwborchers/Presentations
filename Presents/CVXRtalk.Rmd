---
title: "The CVXR Package"
author: "Hans W Borchers, Duale Hochschule Mannheim"
date: "10/04/2018"
output: 
  ioslides_presentation: 
    self_contained: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## What is "Convex Optimization" ?

"**Convex minimization** is a subfield of optimization that studies
the problem of minimizing convex functions over convex sets."  
--Wikipedia

* Very fast algorithms (like Linear Programming, LP)
* Convex problems have only *one* (global) optimum
* Many statistical and engineering applications 
  can be modeled as convex problems

* BUT: May be difficult to find an appropriate 
  convex formulation (NP-hard)


## Convex Functions and Domains

A function $f: \textbf{R}^n \to \textbf{R}$ is *convex* if its domain of definition is convex and for all $x, y$ and $0 \le \theta \le 1$ we have
$$
  f(\theta x + (1-\theta) y) \le \theta f(x) + (1 - \theta) f(y)
$$

![Figure: Graph of a convex function (Boyd et al. 2004)](convex.png)

## What is CVX* ?

CVX\* is a family of implementations of **Disciplined Convex Programming** (DCP), invented an initialized by *Stephen Boyd* and collaborators at Stanford University:

* CVX       (MATLAB, ~2005)
* CVXPY     (Python, 2013)
* convex.jl (Julia, 2015)
* CVXR      (R, 2017)

Disciplined convex programming imposes a set of conventions to follow when constructing convex problems.


## Loading CVXR

```r
devtools::install_github("anqif/CVXR")
vignette("cvxr_intro", package="CVXR")
```

```{r}
# suppressMessages(suppressWarnings(library(CVXR)))
library(CVXR)
```

```r
package?CVXR
```

```{r echo=FALSE}
options(digits=5)
```


## Example: Linear Regression

```{r}
wine <- read.csv("winequality.csv", sep=";")

mod0 <- lm(quality ~ . - 1, data=wine)
unname(coefficients(mod0))
```

```{r}
A <- wine[, 1:11]; b = wine[, 12]
mod00 <- qr.solve(A, b)
unname(mod00)
```


## Linear Regression with CVXR

```{r}
x <- Variable(11)
objective  <- Minimize(sum((b - A %*% x)^2))
problem    <- Problem(objective)
result     <- solve(problem)
c( result$getValue(x) )
```


## Positive Coefficients only

```{r}
x <- Variable(11)
objective  <- Minimize(sum((b - A %*% x)^2))
constraint <- list(x >= 0)
problem    <- Problem(objective, constraint)
result     <- solve(problem)
c( result$getValue(x) )
```


## A 'Sum Equal to 1' Solution

```{r}
x <- Variable(11)
objective  <- Minimize(sum((b - A %*% x)^2))
constraint <- list(x >= 0, sum(x) == 1)
problem    <- Problem(objective, constraint)
result     <- solve(problem)
zapsmall( c( result$getValue(x) ) )
sum(result$getValue(x))
```


## 'Isotonic' Regression

"In statistics, isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence of observations under the [monotone] constraints." -- Wikipedia

Example: `x[1]<=x[2]<=...<=x[n]`

```{r}
x <- Variable(11)
objective  <- Minimize(sum((b - A %*% x)^2))
constraint <- list(diff(x) >= 0)
problem    <- Problem(objective, constraint)
result     <- solve(problem)
c( result$getValue(x) )
```


## L1 Regression

"L1 regression, or Least Absolute Deviations (LAD) regression, is a statistical optimality criterion and the statistical optimization technique that relies on minimizing the L1-norm."

Linear L1 regression: $\quad\textrm{Min!} \, \sum_1^n |\,b - A\,x |$

```{r}
x <- Variable(11)
objective  <- Minimize(sum(abs(b - A %*% x)))
constraint <- list(x[11] == 0)
problem    <- Problem(objective, constraint)
result     <- solve(problem)
c( result$getValue(x) )
```


## Robust Regression

"Robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods, especially high sensitivity to outliers."

**Huber's M-estimation**: $\textrm{Min!} \, \sum L_M(b - A\,x)$
with $L_M(u) = \frac{1}{2} u^2$ if $|u|\lt M$, else $2M|u| - M^2$.

```{r}
M <- 1  # Huber threshold
x <- Variable(11)
objective  <- Minimize(sum(huber(b - A %*% x, M)))
problem    <- Problem(objective)
result     <- solve(problem)
c( result$getValue(x) )
```


## Example: Robust Regression

Stars outer temperature vs. light intensity:

```{r echo=FALSE}
stars = read.csv("starscyg.csv")
with(stars, plot(log.light, log.temp, col="black",
                 main="Rebust Linear Regression"))
grid()

mod1 <- lm(log.temp ~ log.light, data=stars)
abline(mod1, col="darkred")

A = cbind(1, stars$log.light)
b = stars$log.temp

library(CVXR)
M <- 0.2  # Huber threshold
x <- Variable(2)
objective  <- sum(huber(b - A %*% x, M))
problem    <- Problem(Minimize(objective))
result     <- solve(problem)
ab = result$getValue(x)
abline(ab[1], ab[2], col="darkblue")
legend(4.0, 3.8, legend=c("linear","Huber"),
       col=c("darkred", "darkblue"), lty=1)
```


## Example continued ...

```{r}
# library(CVXR)

stars = read.csv("starscyg.csv")
A = cbind(1, stars$log.light)
b = stars$log.temp

M <- 0.2  # Huber threshold
x <- Variable(2)
objective  <- sum(huber(b - A %*% x, M))
problem    <- Problem(Minimize(objective))
result     <- solve(problem)
ab = result$getValue(x)
ab
```


## Example: Piecewise Linear Regression

```{r echo=FALSE}
sp500 = read.csv(file="sp500.csv")
y = sp500$log
N = length(y)   # 2001

plot(1:N, y, type='l', col="darkgray",
     xlab="time [days]", ylab="log(index)",
     sub="March 1999 - March 2007",
     main="Daily Standard & Poor SP500 Index")
grid()

lambda = 40
beta <- Variable(length(y))
objective <- 0.5 * p_norm(y - beta) +
                lambda * p_norm(diff(x = beta, differences = 2), 1)
problem  <- Problem(Minimize(objective))
sol <- solve(problem)$getValue(beta)

lines(1:N, sol, col="darkred", lwd=2)
inds = which(abs(diff(diff(sol))) > 1e-08)
for (i in inds) abline(v=i, col="gray", lty=2)
```


## Example Solved with CVXR

*One* approach to 'piecewise linear regression' is through this formula:
$$
  \textrm{Min!} \frac{1}{2} \sum_1^n (y_i - z_i)^2 + \lambda \sum_1^{n-2}| z_i - 2z_{i+1} + z_{i+2}|
$$

```r
lambda = 40
z <- Variable(length(y))
objective <- 0.5 * p_norm(y - z) +
                lambda * p_norm(diff(z, differences = 2), 1)
problem  <- Problem(Minimize(objective))
sol <- solve(problem)$getValue(z)
```


## Quadratic Optimization

**Quadratic Programming** (QP) is the problem of optimizing a quadratic expression of several variables subject to linear constraints.

$$
\mathrm{Minimize} \quad \frac{1}{2} x^T Q x + c^T x \qquad
s.t. \quad A x \le b
$$

where  
$Q$ is a symmetric, positive (semi-)definite $n \times n$-matrix,  
$c$ an $n$-dim. vector,  
$A$ an $m \times n$-matrix, and  
$b$ an $m$-dim. vector.

For some solvers, linear equality constraints are also allowed.


[Quadratic Optimization](https://cran.r-project.org/web/views/Optimization.html#quadratic-optimization) CRAN Optimization Task

## Example: Smallest Enclosing Ball

Given a set $P = \{p_1, ..., p_n\}$ of n points in $\textrm{R}^d$,
find a point $p_0$ such that $max ||p_i - p_0||$ is minimized.

Known algorithm to solve this as Qudratic Programming task:

Define matrix $C = (p_1, ..., p_n)$, i.e. coordinates of points in 
columns, and minimize the quadratic form
$$
    x^T C^T C x - \sum p_i^T p_i x_i
$$
subject to $\sum x_1 = 1$ and all $x_i >= 0$.

Let $x = (x_1, ..., x_n)$ be an optimal solution, then the linear 
combination $p0 = \sum x_i p_i$ is the center of the smallest 
enclosing ball, and the negative of the minimum value at $x$ is
the square of the radius of the ball.


## Example (continued)

As an example, we will look at finding a smallest circle enclosing 100 randomly given points $p_1, \ldots, p_{100}$ in $\textrm{R}^2$. We will represent the coordinates of these points as columns in the following matrix `P`.

```{r}
set.seed(7531); N <-  100
P <- matrix(10*rnorm(2*N), nrow=2)
# plot(P[1, ], P[2, ], col="red", xlab="", ylab="")
```

```{r}
C <- t(P) %*% P
d <- apply(P^2, 2, sum)
```


## Example Solved with CVXR

```{r}
x           <- Variable(N)
objective   <- Minimize( quad_form(x, C) - sum(d * x))
constraint  <- list(x >= 0, sum(x) == 1)
problem     <- Problem(objective, constraint)
result      <- solve(problem, solver="SCS")     # default: ECOS
```

```{r}
x0 <- result$getValue(x)
p0 <- P %*% x0; c(p0)
r0 <- c(sqrt(sum(colSums(P^2)*x0) - t(x0)%*%t(P)%*%P%*%x0))
r0
```


## Example Solution

```r
plot(P[1, ], P[2, ], xlim=c(-40,40), ylim=c(-40,40), asp=1,
     col = "blue", xlab = "x", ylab = "y",
     main = "Smallest Enclosing Ball")
# ...
```

```{r echo=FALSE}
plot(P[1, ], P[2, ], xlim=c(-40,40), ylim=c(-40,40), asp=1,
     col = "blue", xlab = "x", ylab = "y",
     main = "Smallest Enclosing Ball")
th <- seq(0, 2*pi, length.out=100)
xc <- p0[1] + r0 * cos(th)
yc <- p0[2] + r0 * sin(th)
points(p0[1], p0[2], pch = 'x', col = "red")
lines(xc, yc, col="darkred")
grid()
```


## CVXR Tutorial Examples

Largest Euclidean ball in a 2D polyhedron  
Catenary Problem  
Huber Regression  
Logistic Regression  
Quantile Regression  
Censored Regression  
Isotonic Regression  
Near Isotonic and Near Convex Regression  
L1 Trend Filtering  
Elastic Net  
Saturating Hinges  
Direct Standardization  
Log-Concave Density Estimation  
Sparse Inverse Covariance Estimation  
Kelly Gambling  
Fastest Mixing Markov Chain  
Portfolio Optimization


## Example: Catenary

Solve the "hanging chain curve" as an optimization problem!  
See [hwborchers.lima-city.de/Presents/catenary.html](https://hwborchers.lima-city.de/Presents/catenary.html).

```{r echo=FALSE}
catenary <- read.csv("catenary.csv")
xm <- catenary$x; ym <- catenary$y
xs <- pracma::linspace(0, 1, 100)
ys <- 0.22964*cosh((xs - 0.5)/0.22964) - 0.02603
plot(xs, ys, type='l', col="gray", lwd=5,
     ylim=c(0, 1), xlab='', ylab='',
     main="Catenary Curve of Length 2")
lines(xm, ym, col="red")
lines(c(0,0), c(0,1), lwd=3)
lines(c(1,1), c(0,1), lwd=3)
points(c(0, 1), c(1, 1))
grid()
```


## Catenary Solved with CVXR

```{r}
N <- 100; L <- 2
h <- L / (N-1)
x <- Variable(N)
y <- Variable(N)
objective  <- Minimize(sum(y))
constraint <- list(x[1]==0, x[N]==1, y[1]==1, y[N]==1,
                   diff(x)^2 + diff(y)^2 <= h^2)
problem <- Problem(objective, constraint)
result <- solve(problem)    ## solver="SCS"
xm <- result$getValue(x)
ym <- result$getValue(y)
#  result
## $status:     "optimal"
## $solver:     "ECOS"
## $solve_time: 0.008145835
## $setup_time: 0.000476103
```



## Running Times -- Catenary Example

| Solver | N = 50 | N = 100 | N = 1000 |
|--------|--------|---------|----------|
| auglag | 8.0 | 60 [--] | -- |
| Ipopt |  |  |  |
| CVXR/ECOS | 0.283 | 0.297 | NA |
| CVXR/SCS | 0.311 | 0.330 [-] | 1.141 [--] |
| ECOS | 0.002 | 0.003 | 0.036 |
| SCS | 0.002 | 0.010 | 0.280 |
| Rmosek | 0.004 | 0.005 | 0.033 |
| JuMP | 0.007 | 0.016 | 0.416 |


## Reference

A. Fu, B. Narasimhan, and S. Boyd (2018).*CVXR: An R Package for disciplined convex optimization*. Journal of Statistical Software.  
[To be published.]

<hr />
**Acknowledgements**

The authors would like to thank Trevor Hastie, Robert Tibshirani, John Chambers, David Donoho, and Hans Werner Borchers for their thoughtful advice and comments on this project. We  are  grateful  to  Steven  Diamond,  John  Miller,  and  Paul  Kunsberg  Rosenfield  for  their contributions to the software’s development.  In particular, we are indebted to Steven for his work on CVXPY. Most of CVXR’s code, documentation, and examples were ported from his Python library
<hr />


## Web Links

  - See [anqif/CVXR](https://github.com/anqif/CVXR) on Github

  - CVXR [Package vignette](https://cran.r-project.org/web/packages/CVXR/vignettes/cvxr_intro.html)
  - CVXR [Home page](https://cvxr.rbind.io/)
  - CVXR [Tutorial examples](https://cvxr.rbind.io/post/cvxr_examples/)
  - CVXR [Function reference](https://cvxr.rbind.io/post/cvxr_functions/)

  - Anqi Fu's talk [Disciplined Convex Optimization with CVXR](https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/CVXR-An-R-Package-for-Modeling-Convex-Optimization-Problems) at UseR!2016, Stanford University

  - A. Fu, , B. Narasimhan, and Stephen Boyd. [CVXR: An R Package for Disciplined Convex Optimization](https://web.stanford.edu/~boyd/papers/cvxr_paper.html), Manuscript Draft, 2018.

